{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4)\n"
     ]
    }
   ],
   "source": [
    "# Q Table을 모두 0으로 초기화 : 2차원 (number of state, action space) = (16,4)\n",
    "# 현재 state에서 어떤 action을 취할 때 얻을 수 있는 reward를 저장하는 리스트\n",
    "# Q : 주어진 state에서 어떤 action을 취할 것인가에 대한 길잡이\n",
    "# env.observation_space.n : 환경의 경우의 수\n",
    "# env.action_space.n : 행동의 경우의 수\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "print(Q.shape)\n",
    "# 하이퍼 파라미터 초기화\n",
    "\n",
    "# 할인율(discount) 정의 => 미래의 보상(reward)을 현재의 보상보다 조금 낮게 계산\n",
    "dis = 0.99\n",
    "\n",
    "# 시도 횟수(에피소드)\n",
    "num_episodes = 2000\n",
    "\n",
    "# 에피소드마다 총 보상의 합을 저장하는 리스트\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 업데이트 : Q값은 어떤 상태에서 어떤 액션을 취할 때 얻어지는 보상 + action으로 변화된 state에서 얻을 수 있는 reward의 최대값을 더한다.\n",
    "# 즉, 현재의 보상 + 미래에 가능한 보상의 최대값\n",
    "# 할인율: 미래의 보상에 약간의 패널티를 주는 것\n",
    "# Q(state, action) = Reward + max(Q(new state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 방식\n",
    "\n",
    "frames = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    rAll = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Action 중에 가장 R(Reward)이 큰 Action을 랜덤으로 고르는 방식\n",
    "        # env.action_space.n: 4\n",
    "        # randn(1,4) 1행 4열의 정규분포난수\n",
    "        action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) / (i + 1))\n",
    "        # 해당 Action을 했을 때 environment가 변하고, 새로운 state, reward, done 여부를 반환 받음\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Q = R + 할인율 * max(Q)\n",
    "        Q[state, action] = reward + dis * np.max(Q[new_state, :])\n",
    "        rAll += reward\n",
    "        state = new_state\n",
    "\n",
    "        # 애니메이션을 위하여 정보 기록\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': new_state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        ) \n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E-greedy 방식\n",
    "# 어떠한 확률값 E를 주어 E의 확률로 탐색\n",
    "# E=0.99라면 99%의 확률로 탐색하고 1%의 확률로 개척(새로운 길을 찾음)\n",
    "\n",
    "frames = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    rAll = 0\n",
    "    done = False\n",
    "    \n",
    "    # exploration의 확률\n",
    "    e = 1./((i/100) +1)\n",
    "\n",
    "    # Q learning 알고리즘\n",
    "    while not done:\n",
    "        # E-Greedy 알고리즘으로 action 고르기\n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state, :])\n",
    "\n",
    "        # 해당 Action을 했을 때 environment가 변하고, 새로운 state, reward, done 여부를 반환 받음\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Q = R + Q\n",
    "        Q[state, action] = reward + dis * np.max(Q[new_state, :])\n",
    "        rAll += reward\n",
    "        state = new_state\n",
    "\n",
    "        # 애니메이션을 위하여 정보 기록\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': new_state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        ) \n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate : 0.099\n",
      "Final Q-Table Values\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 5.39113093e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 5.28384742e-06]\n",
      " [0.00000000e+00 0.00000000e+00 6.86175502e-06 6.26833156e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 6.20564825e-06]\n",
      " [5.66896670e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.79313747e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 9.80100000e-01 9.41480149e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Success rate : \" + str(sum(rList) / num_episodes))\n",
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Timestep: 251\n",
      "State: 1\n",
      "Action: 2\n",
      "Reward: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReward: \u001b[39m\u001b[39m{\u001b[39;00mframe[\u001b[39m'\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m         sleep(\u001b[39m.1\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m print_frames(frames)\n",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m, in \u001b[0;36mprint_frames\u001b[1;34m(frames)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAction: \u001b[39m\u001b[39m{\u001b[39;00mframe[\u001b[39m'\u001b[39m\u001b[39maction\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReward: \u001b[39m\u001b[39m{\u001b[39;00mframe[\u001b[39m'\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m sleep(\u001b[39m.1\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
